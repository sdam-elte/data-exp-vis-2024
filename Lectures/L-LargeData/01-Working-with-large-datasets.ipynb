{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Large Datasets\n",
    "\n",
    "As the size of data gets larger more and more problems will occur in our workflow. Either the it will take too much **time**, will not fit into the **memory** of our computer or it will be harder to **comprehend**. \n",
    "\n",
    "The same is true for **visualization**: plotting will take more time and the figure will not be able to tell us the necessary information or might be even misleading the same techniques.\n",
    "\n",
    "In the following notebooks we will address some of these issues following some articles and tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce data size if possible\n",
    "\n",
    "See the [02-Pandas-reduce-data-size](02-Pandas-reduce-data-size.ipynb) notebook!\n",
    "\n",
    "* Store data in the right format\n",
    "* lose some of the data in order to increase the efficiency of your code\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation and python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of package relations\n",
    "\n",
    "Funny site: https://anvaka.github.io/vs/?query=dask\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1024/0*P7Xv__4reO9WuF2I.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas together with Dask\n",
    "\n",
    "* sample of the data\n",
    "\n",
    "https://www.machinelearningplus.com/python/dask-tutorial/:\n",
    "\n",
    "You may use Spark or Hadoop to solve this. But, these are not python environments. This stops you from using numpy, sklearn, pandas, tensorflow, and all the commonly used Python libraries for ML.\n",
    "\n",
    "* Dask tutorial\n",
    "https://medium.com/@gongster/dask-an-introduction-and-tutorial-b42f901bcff5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "follow https://towardsdatascience.com/how-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55follow \n",
    "\n",
    "* https://www.youtube.com/watch?v=Alwgx_1qsj4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas alternatives\n",
    "\n",
    "https://www.datacamp.com/tutorial/high-performance-data-manipulation-in-python-pandas2-vs-polars\n",
    "\n",
    "\n",
    "*    Koalas. A pandas API built on top of PySpark. If you use Spark, you should consider this tool.\n",
    "*    Vaex. A pandas API for out-of-memory computation, great for analyzing big tabular data at a billion rows per second.\n",
    "*    Modin. A pandas API for parallel programming, based on Dask or Ray frameworks for big data projects. If you use Dask or Ray, Modin is a great resource.\n",
    "*    cuDF. Part of the RAPIDS project, cuDF is a pandas-like API for GPU computation that relies on NVIDIA GPUs or other parts of RAPIDS to perform high-speed data manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File formats\n",
    "\n",
    "* HDF5\n",
    "* NetCDF\n",
    "* Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "The underlying problem is that we cannot distinguish between the datapoints because\n",
    "* **the figure is too crowded**\n",
    "* **when using alpha values -> over- or undersaturation**\n",
    "* **saturation cannot be overcome by binning (heatmap)**\n",
    "\n",
    "or the plotting interface is **too slow** to respond **to any interaction**\n",
    "\n",
    "A good introduction:\n",
    "* https://www.slideshare.net/continuumio/visualizing-billions-of-data-points-doing-it-right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datashader\n",
    "<img src=\"https://github.com/holoviz/datashader/blob/main/examples/assets/images/nyc_races.jpg?raw=true\" width=700>\n",
    "\n",
    "With the [holoviews](holoviews.org/) suite interactive exploration of large datasets are possibble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datashading pipeline:\n",
    "1. Select data and project it (e.g. scatter plot)\n",
    "2. Aggregate points into fixed set of bins --> result in one or more scalars\n",
    "3. Transform data using transfer functions --> yields a visible image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T11:40:36.422116Z",
     "start_time": "2020-12-07T11:40:36.290828Z"
    }
   },
   "source": [
    "Example notebooks from the [datashader repository](https://github.com/holoviz/datashader):\n",
    "* **How to Visualize a large dataset**: Datashader-01-Pipeline.ipynb\n",
    "* **Aspects and concepts**: Datashader-02-Interactivity.ipynb\n",
    "* **Networks**: Datashader-03-Networks.ipynb       \n",
    "* **Timeseries example**: Datashader-04-Timeseries.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Datashader\n",
    "https://www.youtube.com/watch?v=n4cFwPan59I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "[Handling Large Dataset](https://medium.com/@keshavaggarwal1311/handling-large-data-sets-the-best-ways-to-read-store-and-analyze-655117d0d939#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImFjM2UzZTU1ODExMWM3YzdhNzVjNWI2NTEzNGQyMmY2M2VlMDA2ZDAiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDA1NzE5MDYzOTY5OTM2MTA4ODciLCJlbWFpbCI6ImplZ2VzbUBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmJmIjoxNzE1MDE5MjI4LCJuYW1lIjoiamVnZXNtIiwicGljdHVyZSI6Imh0dHBzOi8vbGgzLmdvb2dsZXVzZXJjb250ZW50LmNvbS9hL0FDZzhvY0xRNW1ST1hrSGp1SlRaYmpmVzZ1SWVtQ0lHZGN4NERoUmNLd3hDUlVVRVZuaU5uUT1zOTYtYyIsImdpdmVuX25hbWUiOiJqZWdlc20iLCJpYXQiOjE3MTUwMTk1MjgsImV4cCI6MTcxNTAyMzEyOCwianRpIjoiMzJjMGZhN2YwMmRlY2EzMmEzZTFlYjY4OGU3OTg3ZTVjYWM4MWY0OCJ9.kLw4PiVXpypugx3FzgLpb5_xAXM4Gdq9_nTacWzfnD4jBqtg530Xj_oU8mki9-uuoXJogpfWRlUMvaBwmi7SsYO_PPwpiKV1hCN1jhSEVjMZ73y3yEEu4d2lQ6E5YMgQ2tu65KDXdqm-RxeVbXtOJf0mSZ32ZAfD1krRFtwf2wUXuhzHyTFO6tHVKTtwd4OQHkcjxlvxG_R8OipC1HmsbM6AXVb9VB8bxtZR6U3BZiyrCqj6qJFlHwiXonpNWunhqpezrV_1Y7p6okyS1OFlvz2baMb2H-kwum1d5v4oXDkTnSAMB7a_4eOYebBGNcBNU6kYX5Ogcy7iSSW6OZIeHw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
